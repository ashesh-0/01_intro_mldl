{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e36ce0e",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning and Deep Learning\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "The content of this notebook was originally created by Nils Eckstein, Julia Buhmann, and Jan Funke for the 2021 DL@MBL course in Woods Hole, and later chopped up and modified by Florian Jug and Igor Zubarev for the 2022 course DL4MIA.\n",
    "\n",
    "Some code cells will be marked with\n",
    "\n",
    "########################################################################### <br>\n",
    "#######                      START OF YOUR CODE                     ####### <br>\n",
    "########################################################################### <br>\n",
    "\n",
    "... <br>\n",
    "\n",
    "########################################################################### <br>\n",
    "#######                       END OF YOUR CODE                      ####### <br>\n",
    "########################################################################### <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d7fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2135319",
   "metadata": {},
   "source": [
    "# Use Keras to Train a (Small) Neural Network\n",
    "\n",
    "The previous task demonstrated that chosing the weights of a neural network by hand can be quite painful even for simple functions. This will certainly get out of hand once we have more complex networks with several layers and many neurons per layer. But more importantly, the reason why we want to use neural networks to approximate a function is that (in general) we do not know exactly what the function is. We only have data points that describe the function implicitly.\n",
    "\n",
    "In this task, we will design, train, and evaluate a neural network that can classify points of two different classes on a 2D plane, i.e., the input to our network are the coordinates of points in a plane. Net initial network we want to build should have one hidden layer containing 12 nodes (perceptrons) that receive input from two nodes each, and are themselves connected to one single output node that should predict a binary class for the given input (x,y).\n",
    "\n",
    "For training this network, we will create a training and a testing dataset. We will then create the network and use stochastic gradient descent to train all network parameters on the training dataset and evaluate its performance on the testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e130d87",
   "metadata": {},
   "source": [
    "## Generate Training Data...\n",
    "\n",
    "We create both training and testing dataset from the following function (in practice, we would not know this function but have only the data available):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff660ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spiral_data(n_points, noise=1.0):\n",
    "    n = np.sqrt(np.random.rand(n_points,1)) * 780 * (2*np.pi)/360\n",
    "    d1x = -np.cos(n)*n + np.random.rand(n_points,1) * noise\n",
    "    d1y = np.sin(n)*n + np.random.rand(n_points,1) * noise\n",
    "    return (np.vstack((np.hstack((d1x,d1y)),np.hstack((-d1x,-d1y)))), \n",
    "            np.hstack((np.zeros(n_points),np.ones(n_points))))\n",
    "\n",
    "X_train, y_train = generate_spiral_data(100)\n",
    "\n",
    "plt.title('Training set')\n",
    "plt.plot(X_train[y_train==0,0], X_train[y_train==0,1], '.', label='Class 1')\n",
    "plt.plot(X_train[y_train==1,0], X_train[y_train==1,1], '.', label='Class 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "X_test, y_test = generate_spiral_data(1000)\n",
    "\n",
    "plt.title('Test set')\n",
    "plt.plot(X_test[y_test==0,0], X_test[y_test==0,1], '.', label='Class 1')\n",
    "plt.plot(X_test[y_test==1,0], X_test[y_test==1,1], '.', label='Class 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75329bf",
   "metadata": {},
   "source": [
    "# Let's create and train the network on the generated data...\n",
    "\n",
    "Please note how incredible short this code is... that's the power of modern neural network libraries...\n",
    "Try to understand what is roughly going on in the cell below... feel free to use google to figure out what some parts might mean..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7679dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = Sequential()\n",
    "simple_model.add(Dense(12, input_dim=2, activation='tanh'))\n",
    "simple_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "simple_model.compile(loss='mean_squared_error',\n",
    "                optimizer='SGD', # SGD = Stochastic Gradient Descent\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "# Set verbose parameter to 1 to output the training stats\n",
    "simple_model.fit(X_train, y_train, epochs=200, batch_size=32,  verbose=0)\n",
    "\n",
    "# Predict on the testing data\n",
    "prediction = np.round(simple_model.predict(X_test).T[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f2ceb2",
   "metadata": {},
   "source": [
    "Now that training is done, let's plot the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b786bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(pred, model_name):\n",
    "    plt.subplot(1,2,1)\n",
    "\n",
    "    plt.title('Test set')\n",
    "    plt.plot(X_test[y_test==0,0], X_test[y_test==0,1], '.')\n",
    "    plt.plot(X_test[y_test==1,0], X_test[y_test==1,1], '.')\n",
    "    plt.subplot(1,2,2)\n",
    "\n",
    "    plt.title(f'{model_name}_model classification')\n",
    "    plt.plot(X_test[pred==0,0], X_test[pred==0,1], '.')\n",
    "    plt.plot(X_test[pred==1,0], X_test[pred==1,1], '.')\n",
    "    plt.show()\n",
    "\n",
    "plot_results(prediction, 'simple_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc3dd1",
   "metadata": {},
   "source": [
    "And the result is... **BAD!**\n",
    "\n",
    "This is, because the network is likely not powerful enough... hence, the model we used is not adequate for the training task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a8468",
   "metadata": {},
   "source": [
    "## Exercise: find a better solution...\n",
    "\n",
    "Create a second network that is somehow different and performs the task better...\n",
    "\n",
    "**Hint**: Likely it is a good idea to copy the cell from above and somehow change it in a meaningful way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d281a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#######                      START OF YOUR CODE                     #######\n",
    "###########################################################################\n",
    "\n",
    "# create an alternative model (network) here and train it as we did before...\n",
    "# currently good_model is the same as the network from above... just to give you a starting point...\n",
    "\n",
    "good_model = ...\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "#######                       END OF YOUR CODE                      #######\n",
    "###########################################################################\n",
    "\n",
    "# Visualize the architecture \n",
    "good_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba95508",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "Write the configuration and training code in the following cell.\n",
    "\n",
    "**Hint**: You'd need to configure model for training with compile function, provide training data and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1e39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model for training\n",
    "###########################################################################\n",
    "#######                      START OF YOUR CODE                     #######\n",
    "###########################################################################\n",
    "\n",
    "#Hint: You'd need to compile the model with the correct loss function, optimizer and metric...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "\n",
    "#Hint: And train the model using .fit() with correct parameters\n",
    "history = ...\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "#######                       END OF YOUR CODE                      #######\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34fc97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Training Metrics')\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(history.history['loss'])\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "axes[1].plot(history.history['accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24d5eee",
   "metadata": {},
   "source": [
    "# Bonus exercise: A glimpse under the hood\n",
    "\n",
    "Keras `fit()` and `evaluate()` functions provide useful abstraction, which would be enough for many basic tasks. However in some cases, like generative adversarial network(GAN) training, you may need to write a customized training loop. In the following cell you can see what's going on on the low level. \n",
    "<br>\n",
    "<br>\n",
    "You can use the original Tensorflow tutorials(https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough) and [here](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch) as a reference\n",
    "\n",
    "**Hint** You may need to restart the kernel. To correctry run this cell skip the cell with `model.compile()` and `model.fit()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396bfed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "num_epochs = 300\n",
    "batch_size = 10\n",
    "\n",
    "# Create a dataset object using tf.data api \n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "\n",
    "# Define loss function\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Training loop - iterating over number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Define metrics\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.BinaryAccuracy()\n",
    "    \n",
    "    # Iterating over dataset, batch by batch\n",
    "    for x, y in dataset_train.batch(batch_size):\n",
    "        \n",
    "        # Define GradientTape to record the operations during the forward pass\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # Run forward pass on a batch. training is only needs to be set to True if there are layers with \n",
    "            # different behavior during training versus inference (e.g. Dropout).\n",
    "            pred = good_model(x, training=True)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss_value = loss(y_true=y, y_pred=pred)\n",
    "            \n",
    "            # Get the gradients of the trainable parameters with respect to the loss.\n",
    "            grads = tape.gradient(loss_value, good_model.trainable_variables)\n",
    "            \n",
    "            # Run one step of gradient descent by updating the value of the variables to minimize the loss.\n",
    "            optimizer.apply_gradients(zip(grads, good_model.trainable_variables))\n",
    "\n",
    "        # Track progress. # Add current batch loss and accuracy\n",
    "        epoch_loss_avg.update_state(loss_value)\n",
    "        epoch_accuracy.update_state(y, pred)\n",
    "\n",
    "    # End epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch, epoch_loss_avg.result(), epoch_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b2521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Training Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(train_loss_results)\n",
    "\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "axes[1].plot(train_accuracy_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb73a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the testing data and plot the results\n",
    "prediction2 = np.round(good_model.predict(X_test).T[0])\n",
    "\n",
    "plot_results(prediction2, 'good_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354957f9",
   "metadata": {},
   "source": [
    "### Done? Cooool!\n",
    "\n",
    "You should now make a short coding break and visit the website https://playground.tensorflow.org\n",
    "\n",
    "Play a bit with it... figure out what elements are part of a typical network training workflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

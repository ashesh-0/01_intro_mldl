{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bcb77672",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning and Deep Learning\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "The content of this notebook was originally created by Nils Eckstein, Julia Buhmann, and Jan Funke for the 2021 DL@MBL course in Woods Hole, and later chopped up and modified by Florian Jug, Igor Zubarev and Ashesh for the 2022 and 2023 course DL4MIA.\n",
    "\n",
    "Some code cells will be marked with \n",
    "\n",
    "########################################################################### <br>\n",
    "#######                      START OF YOUR CODE                     ####### <br>\n",
    "########################################################################### <br>\n",
    "\n",
    "... <br>\n",
    "\n",
    "########################################################################### <br>\n",
    "#######                       END OF YOUR CODE                      ####### <br>\n",
    "########################################################################### <br>\n",
    "\n",
    "This indicates that you need to find a possible errors in the code or in the function parameters. Or add some code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443a7103",
   "metadata": {},
   "source": [
    "### Let's get the MNIST data...\n",
    "\n",
    "This is one of the most famous and most frequently used datasets of small images of hand-written digits and their corresponding ground-truth classes.\n",
    "\n",
    "In this exercise we will learn to predict the correct class given an image of a hand-written digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae04d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets as dts\n",
    "import torchvision\n",
    "transforms = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])\n",
    "trainset=dts.MNIST(root='./data',train=True,download=True,\n",
    "                   transform=transforms)\n",
    "\n",
    "\n",
    "print('Train data ', len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b50be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should also be run for the super bonus exercise \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Show example data\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(trainset[0][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(trainset[1][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(trainset[2][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(trainset[3][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7641fda5",
   "metadata": {},
   "source": [
    "### Brind data in the shape we need during training...\n",
    "\n",
    "In particular, this cell performs the following:\n",
    " * add a channel dimension to train and test data\n",
    " * normalize the pixel intensities to [0,1]\n",
    " * transform the ground-truth label from a digit (0, ..., 9) to one-hot encoded vectors. (Example: the one-hot encoded vector for digit `3` will become `0001000000`, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f165cd6",
   "metadata": {},
   "source": [
    "### Let's create a network we'd like to train...\n",
    "\n",
    "The one currently implemented in the cell below will turn out to not work so well. Run it anyways, but then come back here and start playing with changing the network architecture and hopefully find a better working model for the task at hand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if GPU is available in torch. We may need to run this part on CPU because of hardware incompability\n",
    "import torch \n",
    "print('CUDA available: ', torch.cuda.is_available())\n",
    "print('PyTorch version: ', torch.__version__)\n",
    "\n",
    "# Assign correct device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fffea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# from torchsummary import summary\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "epochs = 3\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Let's convert numpy arrays to torch tensors and create a Dataset object. \n",
    "# Note, that we don't need to convert to one-hot, but we need to change the order of dimensions \n",
    "# to comply with torch convention [batch_size, channels, H, W]\n",
    "# tensor.permute is applied on a tensor and inputs new order of dimesions\n",
    "\n",
    "# Create a dataset object\n",
    "train_dataset = trainset \n",
    "\n",
    "# Define an iterable dataloader which allows batching\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define a loss function \n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Now let's define a model. PyTorch models and layers inherit from torch.nn.Module\n",
    "class MNIST_Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Classifier, self).__init__()\n",
    "        \n",
    "        # Hint: You need to correct the number of channels in convolutional layers as you did with TensoFlow model above\n",
    "        # Note that you need to set input and output channels separately. \n",
    "        # You can also look at the output shapes in the torchsummary\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(3, 3))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        # Hint: Note that you need to set input and output channels separately and consecutive layers \n",
    "        # should have the same number of filters\n",
    "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3))\n",
    "        self.fc1 = nn.Linear(in_features=800, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Layers are essentially functions. So here we sequantially apply those to the input. \n",
    "        # You can also use print(x.size()) after any step for debug\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # View operation reshapes tensor with -1 meaning all the remaining values will go to that dimension\n",
    "        x = x.view(-1, 32 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create model instance and move it to GPU\n",
    "model = MNIST_Classifier()\n",
    "model.to(device)\n",
    "\n",
    "###########################################################################\n",
    "#######                      START OF YOUR CODE                     #######\n",
    "###########################################################################\n",
    "\n",
    "# Finally let's define an optimizer. Note that we need to provide parameters, \n",
    "# which will be optimized(in our case, all model parameters) and learning rate\n",
    "# Use torch.optim.Adam with learning rate defined above\n",
    "optimizer = ... \n",
    "\n",
    "###########################################################################\n",
    "#######                       END OF YOUR CODE                      #######\n",
    "###########################################################################\n",
    "\n",
    "# Visualize the model. PyTorch doesn't have an in-built model summary. \n",
    "# It's available via a separate torchsummary package. We provide the model itself and the shape of the input tensor\n",
    "# summary(model, (1, 28, 28))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2f37f92",
   "metadata": {},
   "source": [
    "### Task: define a training loop and train the network...\n",
    "Hint: You can take a look at the MLP exercise notebook for inspiration ;) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c499047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over number of epochs\n",
    "for epoch in range(epochs):\n",
    "    train_loss_results = 0\n",
    "    train_accuracy_results = 0\n",
    "    \n",
    "    # Set model to train/eval mode is important for some layers(e.g. Dropout) to behave correctly\n",
    "    model.train()\n",
    "    print(f'\\n----- epoch {epoch} -----')\n",
    "    \n",
    "    # Iterate over the dataloader. Each iteration produces one batch of shape [batch_size, channels, H, W]\n",
    "    for data in train_dataloader:\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Move images and labels to correct device. This operation is only required for gpu training\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Set the gradients of all tensors to zero \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ###########################################################################\n",
    "        #######                      START OF YOUR CODE                     #######\n",
    "        ###########################################################################\n",
    "\n",
    "        # Compute forward pass(calling forward method of a model) for current batch\n",
    "        predictions = ...\n",
    "        \n",
    "        # Calculate the loss value \n",
    "        loss = ... \n",
    "        \n",
    "        ###########################################################################\n",
    "        #######                       END OF YOUR CODE                      #######\n",
    "        ###########################################################################\n",
    "\n",
    "\n",
    "        # Compute the gradient of the loss function w.r.t every model parameter, that has requires_grad=True\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters using the gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss values. We only store the number by calling .item()\n",
    "        train_loss_results += loss.item()\n",
    "        \n",
    "        # Accumulate accuracy by first taking the index of largest logit, \n",
    "        # comparing it with the ground truth labels and summing accross batch\n",
    "        train_accuracy_results += ((predictions.argmax(dim=1) == labels).sum().item())\n",
    "\n",
    "    print(f'Loss: {train_loss_results / len(train_dataloader)}')\n",
    "    print(f'Accuracy: {100 * train_accuracy_results / (batch_size * len(train_dataloader))}')\n",
    "                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6af1921",
   "metadata": {},
   "source": [
    "## Task: Evaluation.\n",
    "Now, evaluate the trained model on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a4b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a test dataset and dataloader\n",
    "test_dataset =dts.MNIST(root='./data',train=False,download=True,\n",
    "                   transform=transforms)\n",
    "\n",
    "print('Test data ', len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ccd919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example data\n",
    "def get_prediction(inp):\n",
    "    inp = inp.to(device).unsqueeze(0)\n",
    "\n",
    "    ###########################################################################\n",
    "    #######                      START OF YOUR CODE                     #######\n",
    "    ###########################################################################\n",
    "\n",
    "    prediction = ...\n",
    "\n",
    "    ###########################################################################\n",
    "    #######                       END OF YOUR CODE                      #######\n",
    "    ###########################################################################\n",
    "\n",
    "\n",
    "    prediction = prediction.argmax(dim=1).item()\n",
    "    return prediction\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "inp_tmp, tar_tmp = test_dataset[0]\n",
    "pred_tmp = get_prediction(inp_tmp)\n",
    "ax = plt.imshow(inp_tmp[0], cmap=plt.get_cmap('gray'))\n",
    "plt.gca().set_title(f'Pred:{pred_tmp}, GT:{tar_tmp}')\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "inp_tmp, tar_tmp = test_dataset[4]\n",
    "pred_tmp = get_prediction(inp_tmp)\n",
    "ax = plt.imshow(inp_tmp[0], cmap=plt.get_cmap('gray'))\n",
    "plt.gca().set_title(f'Pred:{pred_tmp}, GT:{tar_tmp}')\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "inp_tmp, tar_tmp = test_dataset[12]\n",
    "pred_tmp = get_prediction(inp_tmp)\n",
    "ax = plt.imshow(inp_tmp[0], cmap=plt.get_cmap('gray'))\n",
    "plt.gca().set_title(f'Pred:{pred_tmp}, GT:{tar_tmp}')\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "inp_tmp, tar_tmp = test_dataset[21]\n",
    "pred_tmp = get_prediction(inp_tmp)\n",
    "ax = plt.imshow(inp_tmp[0], cmap=plt.get_cmap('gray'))\n",
    "plt.gca().set_title(f'Pred:{pred_tmp}, GT:{tar_tmp}')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42638490",
   "metadata": {},
   "source": [
    "### Now, lets look at some of the errors...\n",
    "If you run the cell below, you will get new errors every time you run it.\n",
    "\n",
    "What can you say about the errors? Are they reasonable? Can you explain why the network made these errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c127d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "error_idx = []\n",
    "all_idx = list(range(len(test_dataset)))\n",
    "np.random.shuffle(all_idx)\n",
    "\n",
    "for idx in all_idx:\n",
    "    inp, tar = test_dataset[idx]\n",
    "    pred = get_prediction(inp)\n",
    "    if pred != tar:\n",
    "        error_idx.append(idx)\n",
    "    if len(error_idx) > 4:\n",
    "        break\n",
    "\n",
    "if len(error_idx) > 0:\n",
    "    plt.subplot(1,4,1)\n",
    "    inp_tmp, tar_tmp = test_dataset[error_idx[0]]\n",
    "    pred_tmp = get_prediction(inp_tmp)\n",
    "    ax = plt.imshow(inp_tmp[0], cmap=plt.get_cmap('gray'))\n",
    "    plt.gca().set_title(f'Pred:{pred_tmp}, GT:{tar_tmp}')\n",
    "\n",
    "if len(error_idx) > 1:\n",
    "    plt.subplot(1,4,2)\n",
    "    inp_tmp, tar_tmp = test_dataset[error_idx[1]]\n",
    "    pred_tmp = get_prediction(inp_tmp)\n",
    "    ax = plt.imshow(inp_tmp[0], cmap=plt.get_cmap('gray'))\n",
    "    plt.gca().set_title(f'Pred:{pred_tmp}, GT:{tar_tmp}')\n",
    "\n",
    "    if len(error_idx) > 2:\n",
    "        plt.subplot(1,4,3)\n",
    "        inp_tmp, tar_tmp = test_dataset[error_idx[2]]\n",
    "        pred_tmp = get_prediction(inp_tmp)\n",
    "        ax = plt.imshow(inp_tmp[0], cmap=plt.get_cmap('gray'))\n",
    "        plt.gca().set_title(f'Pred:{pred_tmp}, GT:{tar_tmp}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7913f1e9",
   "metadata": {},
   "source": [
    "### Implement validation loop in PyTorch\n",
    "\n",
    "Usually, validation is done inside the training loop\n",
    "\n",
    "***Hint:*** Almost all the steps are analogous to the training loop, and there's no gradient calculation! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac9c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define an iterable dataloader which allows batching\n",
    "# Hint: Look for how train_dataloader was created above.\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size) #...\n",
    "\n",
    "test_loss_results = 0\n",
    "test_accuracy_results = 0 \n",
    "    \n",
    "# This context manager is needed to disable gradient calculation for  \n",
    "with torch.no_grad():\n",
    "    for test_data in test_dataloader: #...\n",
    "        test_inputs, test_labels = test_data\n",
    "        test_inputs = test_inputs.to(device)\n",
    "        test_labels = test_labels.to(device)\n",
    "\n",
    "        ###########################################################################\n",
    "        #######                      START OF YOUR CODE                     #######\n",
    "        ###########################################################################\n",
    "\n",
    "        # Compute forward pass\n",
    "        test_predictions = ...\n",
    "\n",
    "        # Calculate loss value\n",
    "        test_loss = ...\n",
    "\n",
    "        test_loss_results += test_loss.item() \n",
    "        test_accuracy_results += ...\n",
    "\n",
    "        ###########################################################################\n",
    "        #######                       END OF YOUR CODE                      #######\n",
    "        ###########################################################################\n",
    "\n",
    "    print(f'Loss: {test_loss_results / len(test_dataloader)}')\n",
    "    print(f'Accuracy: {100 * test_accuracy_results / (batch_size * len(test_dataloader))}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cf3a0a",
   "metadata": {},
   "source": [
    "### Once you're done, please answer these question:\n",
    "\n",
    " * What did you play with, what made the biggest difference?\n",
    " * How many parameters did you end up unsing?\n",
    " * How long did you train the network?\n",
    " * What was the best test-error you got overall?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ad72d",
   "metadata": {},
   "source": [
    "### Congratulations! You've made it to the end "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "afc28755679c774118b3b3af99e405c53d143b6e5ce38cf7ce1e143223a5e16a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
